%\addcontentsline{toc}{part}{Discussion}
\chapter{Discussion}
\label{discussion}

This chapter covers the discussion of the tests results presented in the previous section, number \ref{results}.
It follows the same structure than the last two parts. 
First, the benchmarking of both the nodes and the topics is discussed. 
Afterwards, the results obtained in the accuracy experiment are explained and justified. 
%\\
Chapter \ref{conclusions} is devoted to the improvement of the system based on the observations performed on the experiments. 

\section{Performance testing}
	\paragraph{Package and nodes benchmarking}\mbox{}
		\\

			The nodes with a higher computing consumption are the ROI segmenters and the feature extractors both 2D and 3D. 
			Since the 3D data has a higher size, the usage of the nodes using 3D information is much higher than those processing 2D data. 
			\\
			The learner recognizer node also has a higher consumption than the converter, event handler or system output nodes. 
			This is because the last ones perform simple conversions and computations of integers and floating data. 
			On the other hand, the learner recognizer node implements the state machine of the software. 
			This means it has to deal with a higher amount of data than the previous nodes. 




		\paragraph{Topic benchmarking}\mbox{}\\

			Chapter \ref{results} presented the results of the topic benchmarking. 
			Two different characteristics were measured: the publishing rate and the bandwidth used. 
			That data can be found in the figures \ref{hz} and \ref{bw} respectively. 
			\\

			\begin{itemize}
				\item{\textbf{Publishing rate}}\\

			The publishing rate varies significantly between topics. 
			The first column shows the average number of messages published in each topic. 
			It can be seen that those topics with lower-sized messages experiment a higher average publishing rate. 
			This is due to the reduced processing time and hence delay between message publishes. 
			All topics but one have a more or less similar average publishing rate. 
			\\

			The one different topic is the final object ID. 
			This node transmits the output of the system, as was previously explained in chapter \ref{system_description}.
			The node that publishes it is buffering the object ID topic messages. 
			This creates a delay and the node publishes approximately one message per second in the final object ID topic. 
			This knowledge matches the average rate observed of 0.75 messages per second. 
			It is also confirmed when looking at the next column, in which the minimum time between messages is presented. 
			All the previous topics have a zero or almost zero seconds. 
			But final object ID observes a minimum time of around one second. 
			\\

			The maximum time shows results that increase with the complexity of the processing performed by the node that publishes in that topic. 
			As an example, the segmented and descriptors topics have similar time, around half a second. 
			It is remarkable the value obtained for the object ID topic. 
			It was previously seen that this topic publishes the instant value of the estimated recognized object. 
			The time between messages depend on the previous data transformation required. 
			But also the event that the software is undergoing (learning or recognizing) affects that time. 
			When the system is learning, no estimations of the object ID are performed and hence no messages are published. 
			On the other hand, when the system is recognizing, the topic is filled at a rate of almost thirty messages per second. 
			\\

			This is reflected on the standard deviation column. 
			The values for both object ID and final object ID, which are dependent on the system's event, are much higher than the rest of the topics. 

			\\

			\item{\textbf{Bandwidth}}\\

			The  bandwidth results presented in figure \ref{bw} show the different sizes the data used have. 
			The numbers range from the bytes to the megabytes. 
			The topics using custom messages use a higher bandwidth than those using a standard message such as a number. 
			\\

			This can be illustrated with the final object ID topic. 
			It publishes integer messages and its average bandwidth is around 1 B/s. 
			The next that follows in the lowest bandwidth is the segmented coordinates in pixels. 
			Its messages are vectors composed of two integers. 
			The average bandwidth is around 200B/s. 
			\\

			In the kilobytes range, the hand location, descriptors 2D, event and object ID are located. 
			All but descriptors 2D are filled with custom made messages. 
			The data consists on integers and floats mixed with strings. 
			For more information about custom messages, please read the chapter \ref{system_description}.
			It might be noted that the descriptors 2D topic is an order of magnitude higher than the other ones. 
			This is due to the fact that its messages are images and hence have a higher size than the other ones. 
			\\

			Finally, the megabytes range houses the segmented image topics as well as the segmented point cloud and the descriptors 3D. 
			All are filled with heavy messages that store two-dimensional and three-dimensional data. 
		\end{itemize}

\section{Accuracy measurement}
	This section discusses the results obtained in the accuracy measurement experiment. 
	To consult them please refer to section \ref{results_accuracy_measurement}.
	Each different experiment is discussed in a different subsection.


	\subsection{Template using 1 view}
	Figure \ref{1view_matrix} shows the confusion matrix for the experiment. 
	The diagonal marked in blue color is the fraction of true positives obtained. 
	It can be seen that the software has around a 50\% of success in all objects. 
	Figure \ref{1view_fscore} presents the precision, recall and F1 score for each of the testing objects. 
	\\

	The bottle is the object with a higher success rate. 
	Nevertheless, the system recognized this object a high number of times when all the other objects of the dataset were presented to it. 
	Hence, the precision in the bottle recognition is low, as can be seen in figure \ref{1view_fscore}.
	Also, its recall is low because different objects such as the ball or the mobile were detected when the bottle was being presented to the software.  
	\\

	The ball has a precision and recall of exactly the same value. 
	It is a low value because there were false positives when the ball was being used. 
	The system recognized the bottle and the mobile a large number of times. 
	Also, the output of the system showed a ball when the bottle and the mobile were shown with high rates. 
	It also appeared when the calculator and the skull were presented but a lower and almost negligible amount of times. 
	\\

	The skull presents a very good precision. 
	It is almost negligible the amount of times it was detected when other objects were being held in front of the system. 
	Nevertheless, the recall is worse since the cup and the bottle were outputted a high amount of times when the skull was being used. 
	It could be because all three objects have very similar 2D texture, leading to matchable 2D descriptors. 
	\\

	The cup's results are similar to the ball's. 
	The precision and recall are similar in number and low. 
	The features extracted from this object were not descriptive enough.
	It was confused with the mobile and the bottle, probably due to the similarity in 2D texture as was the case with the skull. 
	Also, the cup was outputted when all the other objects in the dataset were being used, with a specially high ratio in the skull and mobile. 
	\\

	The mobile has even a lower precision than the cup and the results are comparable to the previous ones. 
	The system detected the ball, the cup and the bottle when the mobile was being used a noticeable amount of times.
	Apart from that, the software recognized the mobile when all the other objects but the skull were presented a very high amount of times. 
	\\

	The calculator is the object with the best precision and F1 score of the dataset. 
	It can be seen that it was detected when other objects were shown to the system a negligible amount of times. 
	Nevertheless, when the calculator was being used the system outputted the mobile, the bottle with a high ratio and the cup and the ball with a lower one.
	Since the calculator is a heavily 2D textured object, is possible that a descriptor is very similar to other obtained in the mobile or the object. 
	Also, the 3D shape of calculator, mobile and bottle is similar and could lead to 3D confusions.  


	\subsection{Template using 5 views}
	In figure \ref{5views_matrix} the confusion matrix of this experiment may be seen.
	The figure \ref{5views_fscore} represents the F1 score computation per object.  
	The success fraction represented in the diagonal of figure \ref{5views_matrix} has improved with respect to the previous experiment. 
	Now, the higher rated object is the ball. 
	It can be observed that the system only recognized two different objects when the ball was presented to it: the skull and the mobile. 
	The skull is similar to the ball in its 3D form, which could be the reason behind those false recognitions. 
	The fraction of confusions because of the mobile are negligible. 
	\\
	The skull has around a 0.6 success rate. 
	Most of the 40\% of recognitions were ball. 
	This supports the theory that the similarity between those objects can confuse the algorithms used for the recognition. 
	\\

	The cup has a higher rate than the skull, a 0.66. 
	It is noticeable that the system outputted a cup only when it was shown to it. 
	This leads to the maximum precision possible, 1, which can be seen in figure \ref{5views_fscore}.
	Nevertheless, the system recognized the skull, the bottle and the ball and calculator when the cup was shown to it. 
	The two last objects had a low rate and can be neglected. 
	The skull was recognized a ratio of 0.2, which is high. 
	The reason behind it could be the similarity in the 2D texture of skull and cup. 
	Both are white objects with drawings that contrast with that background. 
	\\

	The bottle has a ratio of 0.69. 
	Both the ball and the skull were outputted when the bottle was shown to the system. 
	The most probable reason is that all three objects have curved sides that could be almost identical in terms of PFH 3D descriptors. 
	Also, the calculator was recognized a negligible amount of times. 
	The bottle was detected when the cup, the mobile and the calculator were presented to the system. 
	Nevertheless, the ratio for all three of them is low, a 0.07. 
	\\

	The mobile has a success of 67\%. 
	It is remarkable that it was detected wrongly when the ball was shown. 
	And in this case, the amount of false recognitions is low, a 0.3. 
	Figure \ref{5views_matrix} shows that when the mobile was shown to the system, the skull, the ball and the bottle were detected. 
	The mobile has rounded corners and sides that could make 3D descriptors on those locations be similar to the previous objects. 
	Also, the mobile is white with details represented in the 2D texture. 
	This feature is common with the bottle and the cup. 
	For certain positions of the cup and the mobile the 2D projection could be similar and lead to obtain less representative descriptors of the object. 
	\\

	Finally, the calculator has a score of 0.62. 
	It was recognized in all previous measurements except when the ball was shown to the system. 
	Nonetheless, the ratio of false positives is negligible ( a 3\%). 
	The objects that were detected when the calculator was presented are the skull, the ball and the bottle. 
	Since the calculator has a very cluttered 2D texture it is possible that some descriptors are similar to the ones extracted in other objects. 
	\\

	Figure \ref{5views_fscore} shows that the object with a higher F1 score is the cup, thanks mainly to its extremely high precision.  
	All testing objects obtained similar scores, around the 70 \% except the skull and the ball. 
	This is because the high 3D similarity between both. 


	\subsection{Template using 10 views}
	The results of this experiment may be found in figures \ref{10views_matrix} and \ref{10views_fscore}. 
	It can be observed an improvement with respect to the previous measurements. 
	Now, all success rates of the objects are above the 69\%. 
	Also it can be seen that the number of false recognitions was reduced noticeably mainly in the bottle, mobile and calculator objects. 
	\\

	The ball is the object with highest success rate, a 93\%. 
	This time it was only confused with the cup a 7\%. 
	When the skull, the calculator and the mobile were presented to the system, the ball was detected a 0.31, 0.21 and 0.07. 
	Also, it was recognized a negligible ratio when the cup and the bottle were shown. 
	Those results are high and may be attributed to the improvable segmentation performed in the system. 
	Possible upgrades are explained in section \ref{conclusions}.
	\\
	
	The skull was detected correctly a 69\% of times. 
	It was only confused with the skull due to their shape similarity already mentioned.
	The skull was also recognized when the cup, the bottle and the mobile were being used. 
	\\
	The cup had a success rate of 76\%. 
	The system recognized wrongly the skull 14 \% and the ball, the mobile and calculator a negligible amount of times. 
	The skull has a similar 2D texture, a white background with highly defined drawings on it. 
	This could be the reason of the system's error. 
	The cup was also detected when the ball and the mobile were shown to the system. 
	\\

	The bottle obtained a 89\% success rate. 
	The system only made a false recognition of the bottle when the calculator was being shown to it, and the ratio is negligigle. 
	The bottle was confused with the skull and the ball, for the sames reasons as the cup. 
	Those objects, the skull, the cup and the bottle have similar 2D features that may be matched wrongly. 
	\\
	The mobile obtained a 0.72 rate. 
	The cup, ball, skull and calculator were recognized badly. 
	The mobile was wrongly recognized only when the cup was being shown to the system and the ratio is negligible (0.3). 
	\\
	Finally, the calculator has a 76\% success rate. 
	The system detected the ball a 21\% of times in this measurement. 
	It is possible that the dense 2D textures of both the ball and the calculator affected the system in the matching process. 
	The calculator was recognized a negligible amount of times when the mobile and the cup were being used. 
	\\

	Figure \ref{10views_fscore} presents much better results than the previous experiments (Figures \ref{1view_fscore} and \ref{5views_fscore}).
	The bottle is the object with a higher F1 score. 
	It is noticeable that the bad precision presented in the ball recognition limited its F1 score result. 


	\subsection{Comparison of the experiments results using different number of views}
	Figures \ref{comparison_success} and \ref{comparison_fscore} present a compilation of all the data extracted from the experiments. 
	In \ref{comparison_success} it can be seen that through the experiments the success rate has increased. 
	The differences between objects are almost identical from one experiment to the other. 
	This demonstrates that the errors of the system due to 2D or 3D similarity between objects may be improved using a higher number of views. 
	\\
	On the other hand, in \ref{comparison_fscore} it can be seen that the improvement with the number of views is not the same as the success rate. 
	It is particular the case of the skull. 
	When the views were increased from one to five, the system outputted the skull more when other objects were being used. 
	This reduced the ball's precision and even though the recall was improved, the final F1 score was affected.  
	The same phenomena occurred with the cup but when the number of views was increased from five to ten. 
	All the other objects obtained normal results. 
	With lower number of views, the F1 score is lower than with higher amount of views. 
	The results of the experiment using ten views are very promising. 
	Further measurements with higher amount of views could be performed to determine the optimum balance between F1 score and processing requirements. 