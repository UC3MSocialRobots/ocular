\chapter{Computer Vision fundamentals}
\label{fundamentals}
This chapter aims to give a global overview of the main computer vision vocabulary and methods related with object learning and recognition. 
Each section present a different processing step. 
The main processing phases of an object learning and recognition system are: 
\begin {enumerate}
	\item{\textbf{Data input: Hardware} \\}
	The data input of the system is performed using a vision acquisition device. 
	In this thesis, an RGB-D (Red, Green, Blue and Depth) sensor is being used. 
	Section \ref{hardware} presents the most used vision sensors in robotic computer vision. 
	% Further explanations may be read in section \ref{hardware}.

	\item{\textbf{Raw input data processing}\\}
	% The raw data that comes out of the sensors is not correctly formatted. 
	Usually, the acquired information needs some preprocessing. 
	This step transforms the input data coming from the sensor into a data structure that is easy to handle and useful for posterior processing.
	A more complete explanation can be found in section \ref{input_data}

	\item{\textbf{Segmentation}\\}
	The next step is to segment the input data, that is, reduce it to the Region Of Interest. 
	For further information please read section \ref{segmentation}.

	\item{\textbf{Object description methods}\\}
	The input data is not suitable to be matched as it is. 
	In order to apply recognition and matching algorithms, the extraction of certain relevant information of each object is required. 
	That extracted information is the one that describes the object and that can be used in matching algorithms. % using different techniques. 
	More information can be found in section \ref{descriptors}.
\end{enumerate}

% The computing phases are ordered in the same way as they are performed in the software developed for this thesis. 
The different phases appear in the same order in the software developed in this thesis. 
Further details about the processing steps may be found in section \ref{system_design}.

%\addcontentsline{toc}{chapter}{Hardware}
\section{Hardware}
\label{hardware}
The advance in the computer vision discipline is greatly linked with the development in the hardware. 
The hardware components present in a computer vision system are the following: 
% The main components of a computer vision system are the following: 

\begin{itemize}
	\item{Power Supply:} Device needed by the other components in order to work. 
	\item{Acquisition device:} Device that captures the world and represents it as an array of data. That data can be two or three dimensional. 
	\item{Processing unit:} Receives the information from the acquisition device and processes it. It is usually programmable. Nowadays the most used processing units are PCs. 
	\item{I/O unit:} Serves as a bridge between the acquisition device and the processing unit if needed. 

\end{itemize}

In this chapter the state of the art of the different acquisition devices is going to be presented. 

\subsection{Acquisition devices}
There are different acquisition devices being used in the object learning and recognition field. 
They are usually classified depending on the output data they provide: 

\begin{itemize}
	\item{Cameras:}	The output data is two-dimensional. 
	\item{RGB-D sensors:} The output data is three-dimensional. 
\end{itemize}

The usage of one or another acquisition device depends on the application. 
The RGB-D sensors provide more information than cameras. 
% They reduce the ambiguities produced by the cameras when projecting the three-dimensional world into two dimensions. 
% But also the RGB-D sensors output a higher amount of data. 
% That is why, using three-dimensional information as the input of a software requires a higher-capacity processing unit than using two-dimensional data.
While RGB Cameras provide information in 2D, RGB-D provide information of the depth of the scene.
This added information makes image processing easier because it reduces the ambiguities of the 2D data. 
Nevertheless, the computation time needed is also much higher. 


%%%%%% CAMERAS %%%%%%
%%\addcontentsline{toc}{section}{Cameras}
\subsection{Cameras}
\label{cameras}
There are many different types of cameras that are used in different applications. 
They might be classified according to the technology of the camera: 
\begin{itemize}
	\item\textbf{{CCD [Charged Coupled Device]:}}\\
	They have a high resolution but have a main drawback: the blooming. 
	The blooming is a saturation of the sensor when a high luminosity appears. 
	The sensor outputs a saturated zone bigger than the actual one, occluding part of the image. 
	\item\textbf{{CMOS [Complementary Metal Oxide Semiconductor]:}}\\
	This type of cameras have a semiconductor technology common to other devices such as memories or processors. 
	This fact makes them cheaper than the previous type. 
	Also, they have a smaller size and it is possible to access only a region of the image. 
	Finally, they do not suffer of blooming as the previous ones. 
\end{itemize}

Dimensions of the sensor: 
\begin{itemize}
	\item\textbf{{Linear:}}\\
	The output of this cameras is a vector of pixels in one dimension. 
	They are used mainly in industrial environments.
	Usually they are mounted in a linear actuator in order to cover a line along the region of interest. 
	They have a high resolution range. 
	\item\textbf{{Matricial:}}\\
	They output a matrix of pixels, two-dimensional. 
	They are used in all kinds of applications, from security to industrial ones. 
	The resolution in this type of cameras is more expensive than the previous type. 
	This means that for applications that need a high level of resolution usually linear cameras are preferred due to its cheapness. 
\end{itemize}


According to the number of sensors: 
\begin{itemize}
	\item\textbf{{1 sensor:}}\\
	This type of cameras usually output a black and white image. 
	Nevertheless, they might have colors if a filter such as the Bayer filter or the RGBW filter is used. 
	Those filters reduce the sensibility of the camera. 
	\item\textbf{{3 sensors: }}\\
	They are color cameras. 
	Usually there is a prism located inside the camera that reflects the image to each sensor. 
	Those sensors output the image in one of the RGB components. 
	Adding up them the final color image is obtained. 
\end{itemize}


The output type: 

\begin{itemize}
	\item\textbf{{Analog:}}\\
	Older cameras use this type of output. 
	Nowadays they are only used in certain applications such as security due to its cheapness. 
	They have higher level of noise than the next type of output. 
	\item\textbf{{Digital:}}\\
	This type of output is the most used currently. 

\end{itemize}

%%%%%% RGB-D SENSORS %%%%%%
\subsection{3D sensors}

Computer vision is a field that needs specific hardware to retrieve a description of the world. 
This description has been done for a number of years in two dimensions. 
3D sensors based on lasers devices existed for a long time but they are expensive. 
This fact diminished their use in the investigation field. 
But this changed when the first version of an affordable RGB-D sensor appeared in 2010: the Kinect. 
This project uses a Kinect RGB-D sensor as the input of the system.
\\

This sensor was designed to be used in games, but developers soon realized the huge potential of the hardware for Computer Vision.  
Now, instead of a two-dimensional information as an input it was possible to have three-dimensional information. 
In November 2010 OpenNI was created. Openni is an open-source software framework that can read the data from RGB-D sensors \cite{openni}.  
That same year, PrimeSense released their open source drivers and motion tracking middleware called NITE\cite{NITE}. 
PrimeSense is a company that manufactures RGB-D sensors and, in fact, the Kinect is based on their depth sensing reference. 
The software released by PrimeSense worked with the Kinect as well. 
From that time on, the OpenSource software related with the kinect has increased as well as the different models of RGB-D sensors available in the market.
The Microsoft corporation finally released the SDK (Software Development Kit) on June, 2011 \cite{kinectSDK}.



\paragraph{RGB-D Sensors or Natural Interaction Devices}\mbox{}\\

\label{rgb-d}

The 3D sensor consists in an infrared projector and an infrared camera. 
The projector creates an infrared pattern that is captured by the camera.
Then, it is compared with the references stored in the device and the depth of each point is estimated. 
Afterwards, the depth data is correlated to a RGB camera. 
The output can be described then as a point cloud, a type of data that consists in 3-dimensional and color information for each point.  
\\

It can be easily seen that the use of the infrared spectrum provides a system that can be easily perturbed. 
As an example, the incandescent light irradiates infrared waves that distorts the output of this type of RGB-D sensors. 
Also, the sun's IR waves affects the measures. 
The RGB-D sensors may be used hence only in interiors. 




\section{Raw RGB-D sensor data processing}
\label{input_data}
The Kinect is the sensor that is the input of the system. 
The output of this device cannot be used directly. 
A processing step performed by a driver is needed. 
In the case of this thesis the OpenNI library is being used. 
Two different computing phases are needed. 
The first is used to transform the raw Kinect's output to a formatted data. 
The second step transforms the formatted data into a more useful structure such as an image or a point cloud. 
These structures are easier to use and they contain a more condensed amount of data than the formatted information obtained in the first phase. 
\\

In this thesis these processing steps are being performed using third-party libraries. 
The formatted output of the first phase is converted into an image and a point cloud. 
An image is a matrix of values in which the RGB (Red, Green, Blue) components of each pixel or point is represented. 
A point cloud is also a matrix similar to the image, but with the difference that each point has also an associated position. 
The output of the kinect is processed to match the information contained by the image and the point cloud. 
This processing outputs a more useful data structure : a point cloud with RGB component, that is, a point cloud with color information in it. 
For further information of this process, please read section \ref{ros_packages}.


\section{Segmentation}
\label{segmentation}
The segmentation consists in the extraction of the Region Of Interest (ROI), that is, separating the interesting parts of the image from the background of the input data. 
The segmentation is performed to remove all the useless information from the input data to the system. 
This results in a reduction of the information size. 
The fact of reducing the data size is advantageous as well because the time expended in later computations is lowered. 
Also, less noise is introduced in the system, increasing its performance. 
\\

The method applied to this step varies with the application. 
For example, for object recognition in a table the segmentation would be performed looking first for a flat surface and then extracting the region around it. 
In the case of this thesis, since the object learning and recognition is done in-hand, these extremities are being tracked. 
First, the hands' center position is obtained and later a fixed square is cropped around it. 
More information about this process can be found in section \ref{nodes}.



\section{Object description methods}
\label{descriptors}
% In this section the most important algorithms regarding object learning and recognition and joint detection and tracking related to this bachelor's thesis are going to be presented.
In order to be able to compare two objects, features or descriptors are extracted from them. 
The definition of feature changes depending on the application and context of the computer vision project. In this thesis, the definition that is going to be used is the following:
a feature or descriptor is an interesting or important characteristic, point or region of an image. 
\\

he application determines which feature describes better the object of study. 
Descriptors may have different forms and characteristics. 
For example, if we want to recognize rectangles, its height could be a descriptor.
The height is not scale invariant, but it is rotation invariant. 
If the application needs an scale independent descriptor, the ratio between the height and the width of the rectangle may be used. 
This simple example shows how a feature is defined when the sample object is known. 
But in this thesis I present a system that must be able to recognize all kind of common objects. 
There is no straight forward method to extract a feature as in the example, because there is no structure or characteristic that is repeated in every object a human can hold. 
\\

Descriptors may be local or general. 
A local descriptor represents a point using the information of the points around it. 
Local features from one object may be almost identical to a local feature from a very different object. 
An example of local feature could be the color map of a region composed of a certain number of pixels. 
The same pattern might be found in different highly-textured objects such as a book and a map. 
In order to overcome this problem, general or global descriptors are computed. 
A general descriptor is usually a set of local descriptors that undergo a transformation. 
For example, the local descriptor previously presented could be extracted from four points of the same object located in the borders. 
The information of the global descriptor would be a combination of the one given by the local descriptors and a parameter relating them, for example the relative distance between them. 
This creates a pattern that is more difficult to reproduce in different objects and hence it is more representative for it. 
Global features are used to obtain a more representative description of the studied object, but they are more computationally expensive. 
\\

The best feature or descriptor is the one that possess a higher repeatability, i.e the ability of obtaining the same output given different inputs. 
In this system, repeatability may be described as the ability of obtaining the same predicted object given different views of it. 
% This feature is useful when making a feature matching to compare two objects, which is the case in this project. 
% Hence, it can be seen that the recognition algorithm has as a bottle neck in its performance: the repeatability of the features used to describe the different objects. 
Recognition algorithms depend directly on the repeatability of the features they use. 
If the descriptors have a high repeatability, the system performs better than in the case of having a lower repeatability. 
\\

Chapter \ref{state_of_the_art} presents a description of the state-of-the-art feature extraction algorithms and the differences between them. 