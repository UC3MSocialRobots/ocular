\section{Design alternatives}
% In the present section the steps that were taken in order to obtain the final system for solving the problem are presented. 
This section presents the steps that were taken to obtain the final system. 
% The intermediate designs are analyzed and their pros and cons evaluated. 
The different intermediate designs are shown and the reasons behind the change from one to another design explained. 
The final design is explained in detail in section \ref{system_design}.
\\

\paragraph{First design}
\mbox{} \\

The first design was based in using a hardware with low cost and high accessibility, a camera. The decision of including that device forced the input information to be two-dimensional. The C++ programming language was selected for its high performance and the OpenCV library was picked to aid in the data processing. It was seen that the two dimensional information was not sufficient to obtain a reliable hand tracking. Also, this task created a high lag due to the high amount of computing it needed. 
\\


\paragraph{Hand tracking problem}
\mbox{} \\

In order to reduce the time spent in the hand tracking, it was decided to take advantage of the 3D capabilities of the RGB-D sensors. 
In this case, a Kinect is used which is a low cost device. 
%The fact of having the information of the absolute position of the points in the space gave the new possibility of segmenting using the depth parameter. 
This type of sensors return a point cloud, a data matrix that presents the objects in front of it using three-dimensional points. 
This fact allowed to segment the scene using the depth parameter, reducing the amount of data that needed processing. 
At this point, it was necessary to create a hand-tracker before starting the project itself, that is, the object learning and recognition nodes. 
Also, the introduction of the new coordinate created a problem. OpenCV is designed to deal only with 2D images, another library with state-of-the-art algorithms was needed. At this point it was decided to use PCL to deal with the 3D input data. 
\\


\paragraph{Code structure}
\mbox{} \\

After coding some test cases it became evident that the lag created by the data processing was large enough to difficult the human-machine interaction. 
If the software was supposed to run on real time, it cannot be sequential. 
It was then when, the decision of distributing the tasks between nodes was made. 
Nevertheless, this decision created a new problem: the communication between nodes. 
This could be solved using pipes, sockets and shared memory but it obviously complicated the code programming. 
\\


\paragraph{Programming Framework}
\mbox{} \\

To overcome this difficulty the Robotic Operating System (ROS) programming framework was introduced in the design.
ROS integrates node communication, threading and many Open Source packages that could be useful. 
%Among those packages, the pi\_tracker was discovered. 
One of those packages is the \textit{pi\_tracker}, that implements a joint tracking software using the ROS tools. 
This software that was already present and available was useful because it solved the problem of the hand tracking. 
The output of the package is a ROS topic with the position of all the user's joints presented in a message. 
Thereupon the ROS framework was used and all the already created code was adapted properly. 
But the object learning phase was not yet defined. 
%Some research on the field revealed various algorithms that needed many input samples and a long off-line period in order to have a good performance. 
\\


\paragraph{Learning}
\mbox{} \\

% The present project is intended as a solution for a use within a household both inside a robot or as a standalone software. 
% The objects inside a house are limited and hence the dataset needed......    Given that the number of different objects that appear is not very high, the dataset would be relatively small. 
% Also, the interaction with the user is a key of the software. 
The interaction between the software and the user is key. 
The system must work on real time and be able to learn new objects as quickly as possible. 
This fact was determinant in choosing the learning method of the software. 
It was decided to implement a template matching which do not compromise the efficiency of the software for small sized datasets and allowed a on-line learning. 
\\

The templates extracted in the code were initially to be only in 2D. 
The 3D information was only to be used in the segmentation of the Region Of Interest from the input raw data. 
But afterwards it was thought that extracting also the features of the point clouds could be helpful in the object recognition task. 
Since now the system has different nodes to perform the processing the lag added would be negligible. 
\\

\paragraph{Feature extraction}
\mbox{} \\
	\label{feature_extraction}

The object recognition is performed matching the descriptors extracted of the objects in the dataset and the new one obtained from the frame of the kinect. 
Since the descriptors are different for 2D and 3D data, two algorithms must be used, one for each information. 
%After some research on the subject, the different methods already explained in the section \ref{state_of_the_art} were discovered and their characteristics evaluated. 
In section \ref{state_of_the_art} the different descriptor extraction methods for 2D and 3D are explained and the differences between them evaluated. 
\\

The feature extraction in 2D is done using the Oriented FAST and Rotated BRIEF (ORB) \cite{orb} algorithm. 
It was chosen because it is faster than the Scale-invariant feature transform (SIFT)\cite{sift} and the Speeded Up Robust Features (SURF)\cite{surf} algorithms extracting the descriptors of the image. 
The drawback it has is that it is not scale invariant. 
Nevertheless, in the program presented in this thesis the user must remain within one and two meters and a half from the Kinect. 
In this range, the size of the object seen from the sensor does not change significantly. 
\\

On the other hand, the algorithm chosen for the 3D descriptor extraction is the Point Feature Histograms (PFH) \cite{Rusu} algorithm. 
Its relation between robustness and extraction speed is compliant with the project's requisites. It is fast and the features extracted are computed using just a few neighboring points. Due to this fact there might appear similar clusters in different input data that produce false positives and negatives. The problem when using point clouds is the high amount of data that needs processing. Since the code is running in a personal computer, the computing capability is limited and hence the descriptor extraction must be as fast as possible. Nowadays the research on the field is centered in creating and improving the performance of the 3D information and hence probably in the following years this method could be substituted by another more efficient.  
